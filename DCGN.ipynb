{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d880cc7c-a76f-471a-be78-318f05a9c7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful imported packages\n",
      "Loaded 81 documents\n",
      "seccessfully completed load document in document variable/list\n",
      "Created 395 text chunks\n",
      "\n",
      "chunking step is over\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218d68f521944bdfad53c3df7eb3d9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HuggingFace Embeddings loaded successfully\n",
      "\n",
      "Loading existing vector store...\n",
      "\n",
      " Vectors Stored Successfully\n",
      "\n",
      "\n",
      " Gemini model imported \n",
      "\n",
      "****************************************\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter your problem statement generate the PCA report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Foxberry\\project\\d_generator\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised DeadlineExceeded: 504 Deadline expired before operation could complete..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised DeadlineExceeded: 504 Deadline expired before operation could complete..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised DeadlineExceeded: 504 Deadline expired before operation could complete..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised DeadlineExceeded: 504 Deadline expired before operation could complete..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== GENERATED DOCUMENT ==========\n",
      "\n",
      "Problem Overview\n",
      "• High-dimensional datasets often contain redundant or noisy information that can obscure meaningful patterns.\n",
      "• Dimensionality reduction is required to simplify the data into a lower-dimensional representation while retaining essential characteristics.\n",
      "• This process is necessary for effectively visualizing complex data and improving computational efficiency.\n",
      "\n",
      "Background\n",
      "• Principal Component Analysis (PCA) is a statistical procedure used to reduce the dimensionality of a dataset.\n",
      "• Its primary purpose in data analysis is to visualize high-dimensional data and remove redundant or noisy components.\n",
      "\n",
      "Requirements\n",
      "• The original features of the dataset must be standardized to ensure they are mean-centered and scaled.\n",
      "• The system must be capable of computing a covariance matrix from the standardized features.\n",
      "• The process requires the ability to perform eigenvalue decomposition on the covariance matrix.\n",
      "• Functional selection criteria must be applied to retain eigenvectors with the largest eigenvalues.\n",
      "\n",
      "Proposed Solution\n",
      "• PCA solves the problem of high dimensionality by projecting the original data onto a lower-dimensional subspace.\n",
      "• The approach involves identifying the principal components that capture the most significant variance and discarding those that contribute less to the overall data structure.\n",
      "\n",
      "Technical Architecture\n",
      "• The computation begins with standardization, where features are adjusted to have zero mean and unit variance.\n",
      "• A covariance matrix is calculated to represent the relationships between the standardized features.\n",
      "• Eigenvalue decomposition is then applied to the covariance matrix to produce eigenvectors and eigenvalues.\n",
      "• The architecture focuses on the selection of eigenvectors corresponding to the largest eigenvalues to represent the data in a reduced space.\n",
      "\n",
      "Implementation Steps\n",
      "• Standardize the original features of the dataset to achieve zero mean and unit variance.\n",
      "• Calculate the covariance matrix of the standardized features.\n",
      "• Decompose the covariance matrix into its respective eigenvectors and eigenvalues.\n",
      "• Select the eigenvectors corresponding to the largest eigenvalues to serve as the principal components.\n",
      "• Discard the eigenvectors associated with smaller eigenvalues.\n",
      "• Project the original data onto the subspace spanned by the selected principal components to create a lower-dimensional representation.\n",
      "\n",
      "Risks and Mitigation\n",
      "• Risk: Discarding eigenvectors with smaller eigenvalues may lead to the loss of some data information.\n",
      "• Mitigation: Retain the eigenvectors corresponding to the largest eigenvalues to ensure that the most significant patterns and variance are preserved.\n",
      "\n",
      "Benefits\n",
      "• Enables the visualization of high-dimensional data in a simplified format.\n",
      "• Removes redundant information from the dataset.\n",
      "• Eliminates noisy data that can interfere with analysis.\n",
      "• Provides a more efficient, lower-dimensional representation of the original data.\n",
      "\n",
      "Conclusion\n",
      "• PCA is an effective method for dimensionality reduction that transforms complex datasets through standardization and eigenvalue decomposition.\n",
      "• By projecting data onto a subspace spanned by the most significant principal components, the solution provides a clear and noise-free representation of the original information.\n"
     ]
    }
   ],
   "source": [
    "#remove the ouptput stars \n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "print(\"successful imported packages\")\n",
    "\n",
    "\n",
    "#set environment for api key\n",
    "\n",
    "load_dotenv(\"google_api_key.env\") #collect api key and save file .env \n",
    "\n",
    "\n",
    "\n",
    "# 1. Load Documents (PDF, DOCX, TXT)\n",
    "\n",
    "documents = []\n",
    "\n",
    "documents.extend(PyPDFLoader(r\"D:\\\\1. study material college\\\\8th sem\\\\ML and AI\\\\ML & AI Notes.pdf\").load())\n",
    "#documents.extend(Docx2txtLoader(\"data/sample.docx\").load())\n",
    "#documents.extend(TextLoader(\"data/sample.txt\").load())\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "#print(documents)\n",
    "\n",
    "print(\"seccessfully completed load document in document variable/list\")\n",
    "\n",
    "\n",
    "\n",
    "# 2. Split Documents into Chunks\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=550,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "#chunk_size=Each chunk will contain at most 550 characters\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} text chunks\")\n",
    "\n",
    "\n",
    "print(\"\\nchunking step is over\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#use hugging face for the create embedding and store the vetctor data \n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "print(\"\\nHuggingFace Embeddings loaded successfully\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#vectors store \n",
    "VECTOR_DIR = \"rag_vector_index\"\n",
    "\n",
    "if os.path.exists(VECTOR_DIR):\n",
    "    print(\"Loading existing vector store...\")\n",
    "    vectorstore = FAISS.load_local(\n",
    "        VECTOR_DIR,\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Creating new vector store...\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    vectorstore.save_local(VECTOR_DIR)\n",
    "    print(\"Vector store created and saved\")\n",
    "\n",
    "\n",
    "print(\"\\n Vectors Stored Successfully\\n\")\n",
    "\n",
    "#prompt to a model\n",
    "\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "\n",
    "\n",
    "STRICT OUTPUT RULES:\n",
    "- Do NOT use stars (*), markdown, or special formatting.\n",
    "- Use plain text headings.\n",
    "- Under each heading, use bullet points starting with '•'.\n",
    "- Keep language formal, clear, and professional.\n",
    "- every section need in bold text format and size=20\n",
    "\n",
    "Use ONLY the information provided below.\n",
    "\n",
    "====================\n",
    "REFERENCE CONTEXT\n",
    "====================\n",
    "{context}\n",
    "\n",
    "====================\n",
    "PROBLEM STATEMENT\n",
    "====================\n",
    "{question}\n",
    "\n",
    "====================\n",
    "DOCUMENT STRUCTURE\n",
    "====================\n",
    "\n",
    "Problem Overview:\n",
    "- Explain the problem clearly\n",
    "- Describe why dimensionality reduction is required\n",
    "\n",
    "Background:\n",
    "- Define Principal Component Analysis (PCA)\n",
    "- Explain its purpose in data analysis\n",
    "\n",
    "Requirements:\n",
    "- List technical and functional requirements as bullet points\n",
    "\n",
    "Proposed Solution:\n",
    "- Explain how PCA solves the problem\n",
    "- Describe the approach conceptually\n",
    "\n",
    "Technical Architecture:\n",
    "- Explain PCA computation steps\n",
    "- Mention eigenvectors, eigenvalues, and variance\n",
    "\n",
    "Implementation Steps:\n",
    "- List step-by-step PCA implementation workflow\n",
    "\n",
    "Risks and Mitigation:\n",
    "- List possible risks\n",
    "- Explain mitigation strategies\n",
    "\n",
    "Benefits:\n",
    "- List advantages of PCA\n",
    "\n",
    "Conclusion:\n",
    "- Summarize the solution clearly\n",
    "\n",
    "Generate a clean, well-structured document following the rules exactly.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# 10. Initialize Gemini LLM\n",
    "#firstly you have need choose LLM model and Create the api key\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-3-flash-preview\",\n",
    "    temperature=0.3\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"\\n Gemini model imported \\n\")\n",
    "print(\"*\"*40)\n",
    "\n",
    "\n",
    "# 11. Create RAG Chain\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever, #retrive the vectors from vector database\n",
    "    chain_type=\"stuff\",#Stuff = concatenate all retrieved documents into ONE prompt\n",
    "    chain_type_kwargs={\"prompt\": document_prompt} # You override the default prompt and You inject your own PromptTemplate\n",
    ")\n",
    "\n",
    "\n",
    "# 12. Ask Question\n",
    "\n",
    "problem_statement = input(\"enter your problem statement : \")\n",
    "\n",
    "\n",
    "# 13. Generate Answer\n",
    "\n",
    "final_document = rag_chain.run(problem_statement)   # \n",
    "\n",
    "print(\"\\n========== GENERATED DOCUMENT ==========\\n\")\n",
    "print(final_document)\n",
    "\n",
    "# Save in file \n",
    "\n",
    "#with open(\"generated_document.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
    "#    f.write(final_document)\n",
    "\n",
    "#print(\"\\nDocument saved as generated_document.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ea1b6-aa0d-4362-a3e0-605ba01a3095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#document saving \n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "\n",
    "def file_save():\n",
    "    doc = Document()\n",
    "    \n",
    "    SECTION_TITLES = {\n",
    "        \"PROBLEM OVERVIEW\",\n",
    "        \"BACKGROUND\",\n",
    "        \"REQUIREMENTS\",\n",
    "        \"PROPOSED SOLUTION\",\n",
    "        \"TECHNICAL ARCHITECTURE\",\n",
    "        \"IMPLEMENTATION STEPS\",\n",
    "        \"RISKS AND MITIGATION\",\n",
    "        \"BENEFITS\",\n",
    "        \"CONCLUSION\"\n",
    "    }\n",
    "    \n",
    "    for line in final_document.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "    \n",
    "        if not line:\n",
    "            doc.add_paragraph(\"\")\n",
    "            continue\n",
    "    \n",
    "        p = doc.add_paragraph()\n",
    "        run = p.add_run(line)\n",
    "    \n",
    "        if line in SECTION_TITLES:\n",
    "            run.bold = True\n",
    "            run.font.size = Pt(20)\n",
    "        else:\n",
    "            run.font.size = Pt(11)\n",
    "    \n",
    "    doc.save(r\"C:\\Users\\91957\\Downloads\\PCA.docx\")# where you want save your file\n",
    "    print(\"successfully save in you file manager, pls check it\")\n",
    "\n",
    "file_save() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d6caa-0097-4c8e-b24b-c80f304ad940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (d_generator)",
   "language": "python",
   "name": "d_generator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
